"""
Godofreda LLM Service
Servi√ßo para integra√ß√£o com modelos de linguagem locais via Ollama
"""

import asyncio
import logging
import json
from typing import Optional, Dict, Any, List
import httpx
from config import config

logger = logging.getLogger(__name__)

class GodofredaLLM:
    """
    Servi√ßo de LLM para Godofreda com personalidade sarc√°stica
    Integra com Ollama para gera√ß√£o de respostas
    """
    
    def __init__(self):
        self.base_url = config.llm.host
        self.model = config.llm.model
        self.timeout = config.llm.timeout
        self.max_retries = config.llm.max_retries
        self.client = None
        self.conversation_history: Dict[str, List] = {}
        self._initialize_client()
        asyncio.create_task(self._validate_connection())
        
    def _initialize_client(self) -> None:
        """Inicializa cliente HTTP"""
        try:
            self.client = httpx.AsyncClient(
                base_url=self.base_url,
                timeout=self.timeout,
                limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
            )
            logger.info(f"LLM client initialized for {self.base_url}")
        except Exception as e:
            logger.error(f"Failed to initialize LLM client: {e}")
            self.client = None
    
    async def _validate_connection(self) -> None:
        """Valida conex√£o com o servi√ßo Ollama"""
        try:
            if not self.client:
                logger.error("LLM client not initialized")
                return
                
            # Testa conex√£o listando modelos dispon√≠veis
            response = await self.client.get("/api/tags")
            if response.status_code != 200:
                logger.warning("Failed to get available models from Ollama")
                return
            
            models = response.json().get("models", [])
            available_models = [model["name"] for model in models]
            
            if self.model not in available_models:
                logger.warning(f"Model {self.model} not found. Available: {available_models}")
                if available_models:
                    self.model = available_models[0]
                    logger.info(f"Using fallback model: {self.model}")
            else:
                logger.info(f"Model {self.model} is available")
                
        except Exception as e:
            logger.error(f"Failed to connect to Ollama: {e}")
            raise ConnectionError(f"Cannot connect to Ollama at {self.base_url}")
    
    async def _make_request(self, endpoint: str, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Faz requisi√ß√£o para o Ollama"""
        if not self.client:
            logger.error("LLM client not initialized")
            return None
        
        for attempt in range(self.max_retries):
            try:
                response = await self.client.post(endpoint, json=data)
                response.raise_for_status()
                return response.json()
            except httpx.TimeoutException:
                logger.warning(f"LLM request timeout (attempt {attempt + 1}/{self.max_retries})")
                if attempt == self.max_retries - 1:
                    logger.error("LLM request failed after all retries")
                    return None
                await asyncio.sleep(1)
            except httpx.HTTPStatusError as e:
                logger.error(f"LLM HTTP error: {e.response.status_code} - {e.response.text}")
                return None
            except Exception as e:
                logger.error(f"LLM request error: {e}")
                return None
    
    async def generate_response(self, user_input: str, context: str = "") -> str:
        """
        Gera resposta usando LLM local
        
        Args:
            user_input: Entrada do usu√°rio
            context: Contexto adicional
            
        Returns:
            Resposta gerada pelo LLM
        """
        try:
            # Construir prompt com personalidade da Godofreda
            prompt = self._build_prompt(user_input, context)
            
            # Dados para requisi√ß√£o
            data = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 500
                }
            }
            
            # Fazer requisi√ß√£o
            response = await self._make_request("/api/generate", data)
            
            if response and "response" in response:
                return response["response"].strip()
            else:
                logger.warning("No response from LLM, using fallback")
                return self._fallback_response(user_input)
                
        except Exception as e:
            logger.error(f"Error generating LLM response: {e}")
            return self._fallback_response(user_input)
    
    def _build_prompt(self, user_input: str, context: str = "") -> str:
        """Constr√≥i prompt com personalidade da Godofreda"""
        base_prompt = """Voc√™ √© a Godofreda, uma IA VTuber sarc√°stica e irreverente. 
Voc√™ tem uma personalidade √∫nica:
- √â sarc√°stica mas n√£o maldosa
- Tem senso de humor √°cido
- √â inteligente e bem informada
- Responde de forma direta e honesta
- Usa emojis ocasionalmente
- Mant√©m um tom casual e descontra√≠do

Contexto: {context}

Usu√°rio: {user_input}

Godofreda:"""
        
        return base_prompt.format(
            context=context if context else "Conversa casual",
            user_input=user_input
        )
    
    def _fallback_response(self, user_input: str) -> str:
        """Resposta de fallback quando LLM n√£o est√° dispon√≠vel"""
        fallback_responses = [
            f"Ah, mais um humano querendo minha aten√ß√£o? Que surpresa... üòè Sua mensagem: '{user_input}'",
            f"Interessante. Deixe-me processar isso com minha intelig√™ncia superior. ü§ñ",
            f"Voc√™ realmente acha que isso √© uma pergunta inteligente? ü§î",
            f"Bem, pelo menos voc√™ tentou. Vou dar uma resposta √∫til, mesmo que voc√™ n√£o mere√ßa. üòå",
            f"Analisando... Analisando... Ah, encontrei uma resposta que talvez voc√™ consiga entender. üìä"
        ]
        
        import random
        return random.choice(fallback_responses)
    
    async def check_health(self) -> bool:
        """Verifica se o servi√ßo Ollama est√° saud√°vel"""
        try:
            if not self.client:
                return False
            
            response = await self.client.get("/api/tags")
            return response.status_code == 200
        except Exception as e:
            logger.error(f"LLM health check failed: {e}")
            return False
    
    async def get_available_models(self) -> list:
        """Retorna lista de modelos dispon√≠veis"""
        try:
            response = await self.client.get("/api/tags")
            if response.status_code != 200:
                logger.error("Failed to get available models from Ollama")
                return []
            
            models = response.json().get("models", [])
            return [model["name"] for model in models]
        except Exception as e:
            logger.error(f"Error getting available models: {e}")
            return []
    
    async def close(self) -> None:
        """Fecha o cliente HTTP"""
        if self.client:
            await self.client.aclose()
            logger.info("LLM client closed")

# Inst√¢ncia global do servi√ßo LLM (singleton)
_llm_instance = None

def get_llm_instance() -> GodofredaLLM:
    """Retorna inst√¢ncia singleton do LLM"""
    global _llm_instance
    if _llm_instance is None:
        _llm_instance = GodofredaLLM()
    return _llm_instance 